# ---------------------- #
#     Import Libraries    #
# ---------------------- #
import os
import time
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import joblib

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils.class_weight import compute_class_weight

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2

# ---------------------- #
#   Fix Thai Font Issue   #
# ---------------------- #
thai_fonts = ['Tahoma', 'Garuda', 'Noto Sans Thai', 'DejaVu Sans']
available_fonts = [os.path.splitext(os.path.basename(f))[0] for f in fm.findSystemFonts()]
for font in thai_fonts:
    if font in available_fonts or font in matplotlib.font_manager.get_font_names():
        plt.rcParams['font.family'] = font
        break
else:
    print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö Font ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Font ‡πÄ‡∏ä‡πà‡∏ô 'Noto Sans Thai'")

# ---------------------- #
#   Configuration         #
# ---------------------- #
RANDOM_STATE = 42
MAX_EPOCHS = 300               # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Epoch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
TARGET_ACCURACY = 0.97
BATCH_SIZE = 64
INITIAL_LR = 0.0005            # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á Learning Rate
MODEL_NAME_H5 = "sign_language_model.h5"
MODEL_NAME_NATIVE = "sign_language_model.keras"  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö native ‡∏Ç‡∏≠‡∏á Keras
SCALER_NAME = "scaler.pkl"
LABEL_ENCODER_NAME = "label_encoder.pkl"

# ---------------------- #
#   Custom Callback       #
# ---------------------- #
class TargetAccuracyCallback(Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs.get('val_accuracy') is not None and logs.get('val_accuracy') >= TARGET_ACCURACY:
            print(f"\nüéØ ‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ {TARGET_ACCURACY*100:.0f}% ‡πÅ‡∏•‡πâ‡∏ß! ‡∏´‡∏¢‡∏∏‡∏î‡∏ù‡∏∂‡∏Å")
            self.model.stop_training = True

# ---------------------- #
#   Data Augmentation     #
# ---------------------- #
def augment_keypoints(data):
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ data ‡∏°‡∏µ shape ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    if data.ndim != 2:
        raise ValueError("Data for augmentation ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏°‡∏¥‡∏ï‡∏¥ (samples, features)")
    
    n_features = data.shape[1]
    if n_features < 14:
        print("‚ö†Ô∏è ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 14 ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ flip ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á")
        flip_indices = list(range(0, n_features, 2))  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤ x ‡∏Ç‡∏≠‡∏á keypoints
    else:
        flip_indices = [0,3,6,9,12]
    
    # Flip ‡πÅ‡∏ô‡∏ß‡∏ô‡∏≠‡∏ô
    flipped_h = data.copy()
    flipped_h[:, flip_indices] *= -1

    # Flip ‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á: ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥ indices ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö y ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 1,4,7,10,13
    if n_features >= 14:
        flipped_v = data.copy()
        flipped_v[:, [1,4,7,10,13]] *= -1
    else:
        flipped_v = data.copy()
        flipped_v[:, 1::2] *= -1

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° noise ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
    noise = np.random.normal(0, 0.003, data.shape)
    
    # ‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ augment ‡πÄ‡∏î‡∏¥‡∏° ‡∏≠‡∏≤‡∏à‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô (shift) ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
    shift = np.roll(data, shift=1, axis=1)  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£ shift ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    return np.concatenate([data + noise, flipped_h, flipped_v, shift])

# ---------------------- #
#   Main Function         #
# ---------------------- #
def main():
    try:
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        dataset_path = "dataset.csv"
        if not os.path.exists(dataset_path):
            raise FileNotFoundError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå dataset.csv")
        
        # 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î na_values
        df = pd.read_csv(
            dataset_path,
            sep=',',
            header=0,
            na_values=['U', 'error_value', '#', '...', 'nan', ''],
            encoding='utf-8'
        )
        print("\n‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°: {len(df)} ‡πÅ‡∏ñ‡∏ß")
        print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features: {len(df.columns)-1} (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° label)")
        print("\nüìú ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 3 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å:")
        print(df.head(3))
        
        # 3. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ fillna ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df.fillna(method='ffill', inplace=True)
        df.fillna(0, inplace=True)
        print(f"\nüõ† ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏° Missing Values: {len(df)} ‡πÅ‡∏ñ‡∏ß")
        
        if 'label' not in df.columns:
            raise KeyError("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'label'")
        
        # 4. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™
        print("\nüìä ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™:")
        class_dist = df['label'].value_counts()
        print(class_dist)
        
        plt.figure(figsize=(12,6))
        sns.countplot(
            x=df['label'],
            hue=df['label'],
            palette='viridis',
            legend=False
        )
        plt.title('‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', fontsize=16, pad=20)
        plt.xlabel('‡∏Ñ‡∏•‡∏≤‡∏™', fontsize=14)
        plt.ylabel('‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á', fontsize=14)
        plt.xticks(rotation=45, ha='right')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.show()
        
        # 5. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
        le = LabelEncoder()
        y = df["label"].values
        y_encoded = le.fit_transform(y)
        joblib.dump(le, LABEL_ENCODER_NAME)
        
        # 6. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Class Weights
        class_weights = compute_class_weight(
            class_weight='balanced',
            classes=np.unique(y_encoded),
            y=y_encoded
        )
        class_weights = dict(enumerate(class_weights))
        
        # 7. ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Training ‡πÅ‡∏•‡∏∞ Testing
        X = df.drop("label", axis=1).values
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded,
            test_size=0.2,
            random_state=RANDOM_STATE,
            stratify=y_encoded
        )
        
        # 8. ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Standardization)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        joblib.dump(scaler, SCALER_NAME)
        
        # 9. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Augmentation ‡πÉ‡∏ô Training Set
        try:
            X_train_augmented = augment_keypoints(X_train_scaled)
            X_train_final = np.concatenate([X_train_scaled, X_train_augmented])
            # replicate labels 5 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏° + 4 ‡∏ä‡∏∏‡∏î augment)
            y_train_final = np.concatenate([y_train, y_train, y_train, y_train, y_train])
            print(f"\nüîÄ Training data ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≤‡∏Å {X_train_scaled.shape[0]} ‡πÄ‡∏õ‡πá‡∏ô {X_train_final.shape[0]} ‡πÅ‡∏ñ‡∏ß ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£ Augmentation")
        except Exception as aug_ex:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ augment ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ, ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Training ‡πÄ‡∏î‡∏¥‡∏°‡πÅ‡∏ó‡∏ô:", str(aug_ex))
            X_train_final = X_train_scaled
            y_train_final = y_train
        
        # 10. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡πÅ‡∏•‡∏∞ capacity ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô)
        model = Sequential([
            Input(shape=(X_train.shape[1],)),
            Dense(512, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.3),
            Dense(512, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.3),
            Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.25),
            Dense(128, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.2),
            Dense(len(le.classes_), activation='softmax')
        ])
        
        # 11. ‡∏Ñ‡∏≠‡∏°‡πÑ‡∏û‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•
        optimizer = Adam(learning_rate=INITIAL_LR)
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # 12. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Callbacks
        early_stop = EarlyStopping(
            monitor='val_accuracy',
            patience=25,
            mode='max',
            restore_best_weights=True
        )
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.8,
            patience=15,
            min_lr=1e-6
        )
        target_acc_cb = TargetAccuracyCallback()
        
        # 13. ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        print("\nüöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•...")
        start_time = time.time()
        history = model.fit(
            X_train_final, y_train_final,
            epochs=MAX_EPOCHS,
            batch_size=BATCH_SIZE,
            validation_data=(X_test_scaled, y_test),
            callbacks=[target_acc_cb, early_stop, reduce_lr],
            class_weight=class_weights,
            verbose=1
        )
        training_time = time.time() - start_time
        
        # 14. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö H5 ‡πÅ‡∏•‡∏∞ native ‡∏Ç‡∏≠‡∏á Keras
        model.save(MODEL_NAME_H5)
        model.save(MODEL_NAME_NATIVE)
        print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:")
        print(f"- ‡πÇ‡∏°‡πÄ‡∏î‡∏• H5: {MODEL_NAME_H5}")
        print(f"- ‡πÇ‡∏°‡πÄ‡∏î‡∏• Native: {MODEL_NAME_NATIVE}")
        print(f"- Scaler: {SCALER_NAME}")
        print(f"- Label Encoder: {LABEL_ENCODER_NAME}")
        
        # 15. Visualization: ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÅ‡∏•‡∏∞ Loss
        plt.figure(figsize=(18, 8))
        # ‡∏Å‡∏£‡∏≤‡∏ü Accuracy
        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'], label='‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ù‡∏∂‡∏Å', linewidth=2, marker='o', markersize=5)
        plt.plot(history.history['val_accuracy'], label='‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö', linewidth=2, marker='s', markersize=5)
        plt.title('‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥', fontsize=16, pad=20)
        plt.xlabel('Epoch', fontsize=14)
        plt.ylabel('‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥', fontsize=14)
        plt.ylim(0.5, 1.0)
        plt.axhline(y=TARGET_ACCURACY, color='r', linestyle='--', label='‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢')
        plt.legend(fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.7)
        
        # ‡∏Å‡∏£‡∏≤‡∏ü Loss
        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'], label='Loss ‡∏ù‡∏∂‡∏Å', linewidth=2, marker='o', markersize=5)
        plt.plot(history.history['val_loss'], label='Loss ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö', linewidth=2, marker='s', markersize=5)
        plt.title('‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£ Loss', fontsize=16, pad=20)
        plt.xlabel('Epoch', fontsize=14)
        plt.ylabel('Loss', fontsize=14)
        plt.legend(fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        plt.show()
        
        # 16. ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
        best_epoch = np.argmax(history.history['val_accuracy'])
        print("\nüìú ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å:")
        print(f"- ‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å: {training_time//60:.0f} ‡∏ô‡∏≤‡∏ó‡∏µ {training_time%60:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        print(f"- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Epoch ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(history.history['accuracy'])}")
        print(f"- Epoch ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {best_epoch+1}")
        print(f"- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (Validation): {max(history.history['val_accuracy'])*100:.2f}%")
        print(f"- Loss ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î (Validation): {min(history.history['val_loss']):.4f}")
        
        # ‡πÄ‡∏Å‡πá‡∏ö learning rate ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ attribute 'learning_rate'
        lr_val = model.optimizer.learning_rate
        if hasattr(lr_val, 'numpy'):
            lr_val = lr_val.numpy()
        print(f"- Learning Rate ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: {lr_val:.6f}")
        
        # 17. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö
        test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)
        print("\nüß™ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢:")
        print(f"- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {test_acc*100:.2f}%")
        print(f"- Loss ‡∏ö‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {test_loss:.4f}")
        
    except Exception as e:
        print("\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î:", str(e))
        print("\nüîß ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:")
        print("- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞ encoding ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        print("- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'label' ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        print("- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Missing Values ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Augmentation")

if __name__ == "__main__":
    main()
